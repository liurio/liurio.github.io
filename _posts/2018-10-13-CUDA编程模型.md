```C++
    __device__    由GPU调用，GPU执行
    __global__    由CPU调用，GPU执行
    __host__      由CPU调用，CPU执行
```
# 核函数的定义
```c++
    __global__ void KernelName(Argument List)
        {Function body}
```
核函数必须遵循以下规则：
- 只能访问内存设备
- 必须具有void的返回类型
- 不支持可变数量的参数
- 显示为异步行为
# 核函数的调用
格式：
```c++
    KernelName<<<Grid,Block>>>(Argument List)
```
## 内存函数
- 设备端分配内存 `cudaError_t cudaMalloc(void **devPtr, size_t size)`
- 设备端释放内存 `cudaError_t cudaFree(void** devPtr)`
- 设备端与主机端的传输 `cudaError_t cudaMemcpy(void* dst, const void* src, size_t count, cudaMemcpyKind kind)`
    - kind有两种：`cudaMemcpyHostToDevice`, 主机端向设备端传输
    - `cudaMemcpyDeviceToHost`, 设备端向主机端传输
- 托管内存
    - 因为单独在host和device上进行内存分配，并且要进行数据拷贝，很容易出错，cuda6.0引入统一内存机制。即统一内存使用一个托管来共同管理host和device中的内存，并且自动在host和device之间进行数据传输。
    - 使用`cudaError_t cudaMallocManaged(void **devPtr, size_t size, unsigned int flag=0)`
    - **注意：kernel的执行和host是异步的，由于托管内存自动进行数据传输，所以要用`cudaDeviceSynchronize()`保证device和host的同步。**

# GPU线程管理
当主机端启动一个内核函数kernel function，内核函数的执行会转移到设备上，主机与设备异步执行。此时设备端会启动大量的线程开始并行计算。线程的层次结构由以下部分组成：

- 网格**Grid**：由一个内核启动所产生的所有线程称为一个网格(Grid)。同一个网格中所有线程共享全局内存。

- 线程块**block**: SM负责分配片上的存储资源、寄存器资源等硬件资源给每个线程块。线程块进一步分解为线程束warp，线程束的分解由CUDA自行完成。

- **线程**：线程块中同步执行部分，通过同步和共享内存的方式实现。

- **线程维度**
    - blockDim: 线程块的维度，表示一个线程块block有多少个线程。
    - gridDim：线程网格的维度，表示一个网格有多少个线程块block。
    - **block和grid都是三维向量(x,y,z)**
- **线程id：每个线程有唯一一个id确定身份**
    - blockIdx: 线程块block在网格grid中的位置。
    - threaIdx：线程thread在线程块block中的位置。
    - blockIdx.x：线程块向量的第一维坐标
    - blockIdx.y：线程块向量的第二维坐标；
    - blockIdx.z：线程块向量的第三维坐标。
    - threadIdx.x：线程向量的第一维坐标；
    - threadIdx.y：线程向量的第二维坐标；
    - threadIdx.z：线程向量的第三维坐标。

例如：定义了一个3行2列block的网格grid，并且每个线程块有5行3列个线程

![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/gpu-thread.png)

一个线程需要两个内置的变量(blockIdx,threadIdx)唯一确定，都是dim3类型变量，如图中的Thread(1,1)满足：
```c
    threadIdx.x = 1
    threadIdx.y = 1
    blockIdx.x = 1
    blockIdx.y = 1
```
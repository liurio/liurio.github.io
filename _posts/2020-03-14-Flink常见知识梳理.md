---
logout: post
title: Flink常见知识梳理
tags: [flink,all]
---

###1 广播流怎么实现。具体哪些步骤，实现哪几个方法？

1. 定义一个MapStateDescriptor来描述广播地址.

```java
private state final MapStateDescriptor<String,TableSchema> map = new MapStateDescriptor<>(
	"broadcast",
    BasicTypeInfo.STRING_TYPE_INFO,
    TypeInformation.of(new TypeHint<TableSchema>(){})
);
```

2. 生成数据流和广播流

```java
KeyedStream<String,String> keyedStream = env.addSource(..);
BroadcastStream<TableSchema> broadcastStream = env.addSource(..).returns(TableSchema.class).broadcast(map);
```

3. 连接广播流和数据流

```java
keyedStream.connect(broadcastStream)
```

4. 实现连接的处理方法

```java
keyedStream.connect(broadcastStream)
    .process(new KeyedBroadcastProcessFunction<String, String, TableSchema, String>(){
            @Override
        public void processBroadcastElement(TableSchema value, Context ctx, Collector<String> out){
            //获取旧的值
            TableSchema old = ctx.getBroadcastState(mapStateDescriptor).get("id");
            System.out.println("old value:"+old+",new value:"+value);
            //更新新的值
            state.put("id", value);
        }
            @Override
        public void processElement(String value, ReadOnlyContext ctx,
                                   Collector<String> out) {
            //获取上述更新后的最新值
            TableSchema meta = ctx.getBroadcastState(mapStateDescriptor).get("id");
            /**
            	具体处理逻辑...
            */
        }
    });
```

[https://liurio.github.io/2019/12/21/Flink-Broadcast-State%E5%92%8CSpark-Streaming-Broadcast/](https://liurio.github.io/2019/12/21/Flink-Broadcast-State%E5%92%8CSpark-Streaming-Broadcast/)

### 2 flink消费kafka数据，读字符串的时候怎么指定用avro反序列化 ，还是默认就是，或者是怎么读取Avro的kafka数据?

> 方法1： flink的反序列化schema，FlinkKafkaConsumer011<T>("topic",new DeserializationSchema(......));

> 方法2：直接当做object对象处理，转成string类型object.toString()

### 3 RocksDBStateBackend 已经开启增量备份为啥每隔3次就会一次全量？

flink 还是全量的，增量的意思是，checkpoint 文件，增量上传到 hdfs

### 4 flink sink到hdfs，用BucketingSink还是StreamingFileSink好？


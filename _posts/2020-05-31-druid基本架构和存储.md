---
logout: post
title: druid基本架构和存储
tags: [druid,all]
---

Druid是面向海量数据的，用于实时查询与分析的OLAP存储系统。Druid的四大关键特性如下：

- 亚秒级的OLAP查询分析。Druid采用了列式存储、倒排索引、位图索引等技术，能够在亚秒级别内完成海量数据的过滤、聚合以及多维分析等操作。
- 实时流数据分析。区别于传统分析型数据库采用的批量导入数据进行分析的方式，Druid提供了实时流数据分析，采用LSM(Long structure merge)-Tree结构使Druid拥有极高的实时写入性能；同时实现了实时数据在亚秒级内的可视化。
- 丰富的数据分析功能。针对不同用户群体，Druid提供了友好的可视化界面、类SQL查询语言以及REST 查询接口。
- 可用性与高可拓展性。Druid采用分布式、SN(share-nothing)架构，管理类节点可配置HA，工作节点功能单一，不相互依赖，这些特性都使得Druid集群在管理、容错、灾备、扩容等方面变得十分简单。

## Druid架构

Druid拥有一个多进程，分布式架构。每个Druid流程类型都可以独立配置和扩展，并且一个组件的中断不会立即影响其他组件。

![img](https://gitee.com/liurio/image_save/raw/master/druid/Druid架构图.png)

Druid集群包含多种节点类型，Historical Node、Coordinator Node、Broker Node、Indexing Service Node（包括Overlord、MiddleManager和Peon）以及Realtime Node（包括Firehose和Plumber）。

Druid将整个集群切分成上述角色，有两个目的：

1. 划分Historical Node和Realtime Node，是将历史数据的加载与实时流数据处理切割开来，因为两者都需要占用大量的内存与CPU.
2. 划分Coordinator Node和Broker Node，将查询需求与数据如何在集群内分布的需求切割开来，确保用户的查询请求不会影响数据在集群内的分布情况，从而不会造成数据“冷热不均”，局部过热，影响查询性能。

### 数据查询请求过程

1. 实时数据到达Realtime Node，经过indexing service，在时间窗口内的数据会停留在Realtime Node内存中，而时间窗口外的数据会组织成Segment存储到Deep Storage中；
2. 批量数据经过Indexing Service也会被组织成Segment存储到DeepStorage中；
3. segment的元信息都会被注册到元信息库中，coordinator Nodes会定期同步元信息库，感知新生成的segment，并通知在线的historical node取加载segment，zookeeper会更新整个集群内部数据分布拓扑图。

![https](https://gitee.com/liurio/image_save/raw/master/druid/查询过程.jpg)

当用户需要查询信息时，会将请求提交给Broker Node，Broker Node会请求zk获取集群内数据分布拓扑图，从而知晓请求发给哪些Historical Node以及Realtime Node，汇总各节点的返回数据并将最终结果返回给用户。

### Druid集群各节点

#### Historical Node

- Historical Node的职责单一，就是负责加载Druid中非实时窗口内且满足加载规则的所有历史数据的Segment。每一个Historical Node只与Zookeeper保持同步，不与其他类型节点或者其他Historical Node进行通信。
- Coordinator Nodes会定期（默认为1分钟）去同步元信息库，感知新生成的Segment，将待加载的Segment信息保存在Zookeeper中在线的Historical Nodes的load queue目录下，当Historical Node感知到需要加载新的Segment时，首先会去本地磁盘目录下查找该Segment是否已下载，如果没有，则会从Zookeeper中下载待加载Segment的元信息，此元信息包括Segment存储在何处、如何解压以及如何如理该Segment。Historical Node使用内存文件映射方式将index.zip中的XXXXX.smoosh文件加载到内存中，并在Zookeeper中本节点的served segments目录下声明该Segment已被加载，从而该Segment可以被查询。对于重新上线的Historical Node，在完成启动后，也会扫描本地存储路径，将所有扫描到的Segment加载如内存，使其能够被查询。

![img](https://gitee.com/liurio/image_save/raw/master/druid/historical_node.png)

#### Broker Node

- Broker Node是整个集群查询的入口，作为查询路由角色，Broker Node感知Zookeeper上保存的集群内所有已发布的Segment的元信息，即每个Segment保存在哪些存储节点上，Broker Node为Zookeeper中每个dataSource创建一个timeline，timeline按照时间顺序描述了每个Segment的存放位置。我们知道，每个查询请求都会包含dataSource以及interval信息，Broker Node根据这两项信息去查找timeline中所有满足条件的Segment所对应的存储节点，并将查询请求发往对应的节点.
- Broker会有一个LRU(高速缓存失效策略)，来缓存每Segment的结果。这个缓存可以是本地缓存，也可以借助外部缓存系统(比如memcached)，第三方缓存可以在所有broker中共享Segment结果。当Borker接收到查询请求后，会首先查看本地是否有对应的查询数据，对于不存在的Segment数据，会将请求转发给Historical节点。

![img](https://gitee.com/liurio/image_save/raw/master/druid/broker_node.png)

#### Coordinator Node

- Coordinator Node负责Druid集群中Segment的管理与发布，包括新Segment、丢弃不符合规则的segment、管理segment副本以及segment负载均衡等。如果集群中存在多个Coordinator Node，则通过选举算法产生Leader，其他Follower作为备份。
- Coordinator会定期（默认一分钟）同步Zookeeper中整个集群的数据拓扑图、元信息库中所有有效的Segment信息以及规则库，从而决定下一步应该做什么。对于有效且未分配的Segment，Coordinator Node首先按照Historical Node的容量进行倒序排序，即最少容量拥有最高优先级，新的Segment会优先分配到高优先级的Historical Node上。由之前介绍我们知道，Coordinator Node不会直接与Historical Node打交道，而是在Zookeeper中Historical Node对应的load queue目录下创建待加载Segment的临时信息，等待Historical Node去加载该Segment。
- Coordinator在每次启动后都会对比Zookeeper中保存的当前数据拓扑图以及元信息库中保存的数据信息，所有在集群中已被加载的、却在元信息库中标记为失效或者不存在的Segment会被Coordinator Node记录在remove list中，同一Segment对应的新旧version，旧version的Segments同样也会被放入到remove list中，最终被逻辑丢弃。
- 对于离线的Historical Node，Coordinator Node会默认该Historical Node上所有的Segment已失效，从而通知集群内的其他Historical Node去加载该Segment。但是，在生产环境中，我们会遇到机器临时下线，Historical Node在很短时间内恢复服务的情况，那么如此“简单粗暴”的策略势必会加重整个集群内的网络负载。对于这种场景，Coordinator会为集群内所有已丢弃的Segment保存一个生存时间(lifetime)，这个生存时间表示Coordinator Node在该Segment被标记为丢弃后，允许不被重新分配最长等待时间，如果该Historical Node在该时间内重新上线，则Segment会被重新置为有效，如果超过该时间则会按照加载规则重新分配到其他Historical Node上。
- 考虑一种最极端的情况，如果集群内所有的Coordinator Node都停止服务，整个集群对外依然有效，不过新Segment不会被加载，过期的Segment也不会被丢弃，即整个集群内的数据拓扑会一直保持不变，直到新的Coordinator Node服务上线。

#### Indexing Service

Indexing Service是负责“生产”Segment的高可用、分布式、Master/Slave架构服务。要由三类组件构成：负责运行索引任务(indexing task)的**Peon**，负责控制Peon的**MiddleManager**，负责任务分发给MiddleManager的**Overlord**；三者的关系可以解释为：Overlord是MiddleManager的Master，而MiddleManager又是Peon的Master。其中，Overlord和MiddleManager可以分布式部署，但是Peon和MiddleManager默认在同一台机器上。图-3.5给出了Indexing Service的整体架构。

![https://gitee.com/liurio/image_save/raw/master/druid/indexing%E6%9E%B6%E6%9E%84.png](https://gitee.com/liurio/image_save/raw/master/druid/indexing架构.png)

##### Overlord

- Overlord负责接受任务、协调任务的分配、创建任务锁以及收集、返回任务运行状态给调用者。当集群中有多个Overlord时，则通过选举算法产生Leader，其他Follower作为备份。
- Overlord可以运行在local（默认）和remote两种模式下，如果运行在local模式下，则Overlord也负责Peon的创建与运行工作，当运行在remote模式下时，Overlord和MiddleManager各司其职，根据图3.6所示，Overlord接受实时/批量数据流产生的索引任务，将任务信息注册到Zookeeper的/task目录下所有在线的MiddleManager对应的目录中，由MiddleManager去感知产生的新任务，同时每个索引任务的状态又会由Peon定期同步到Zookeeper中/Status目录，供Overlord感知当前所有索引任务的运行状况。
- Overlord对外提供可视化界面，通过访问http://ip/console.html，我们可以观察到集群内目前正在运行的所有索引任务、可用的Peon以及近期Peon完成的所有成功或者失败的索引任务。

##### MiddleManager

- MiddleManager负责接收Overlord分配的索引任务，同时创建新的进程用于启动Peon来执行索引任务，每一个MiddleManager可以运行多个Peon实例
- 在运行MiddleManager实例的机器上，我们可以在${ java.io.tmpdir}目录下观察到以XXX_index_XXX开头的目录，每一个目录都对应一个Peon实例；同时restore.json文件中保存着当前所有运行着的索引任务信息，一方面用于记录任务状态，另一方面如果MiddleManager崩溃，可以利用该文件重启索引任务

##### Peon

- Peon是Indexing Service的最小工作单元，也是索引任务的具体执行者，所有当前正在运行的Peon任务都可以通过Overlord提供的web可视化界面进行访问

#### RealTime Node

在流式处理领域，有两种数据处理模式，一种为Stream Push，另一种为Stream Pull。

- **Stream Pull**  如果Druid以Stream Pull方式自主地从外部数据源拉取数据从而生成Indexing Service Tasks，我们则需要建立Real-Time Node。Real-Time Node主要包含两大“工厂”：一个是连接流式数据源、负责数据接入的Firehose（中文翻译为水管，很形象地描述了该组件的职责）；另一个是负责Segment发布与转移的Plumber（中文翻译为搬运工，同样也十分形象地描述了该组件的职责）。在Druid源代码中，这两个组件都是抽象工厂方法，使用者可以根据自己的需求创建不同类型的Firehose或者Plumber。Firehose和Plumber给我的感觉，更类似于Kafka_0.9.0版本后发布的Kafka Connect框架，Firehose类似于Kafka Connect Source，定义了数据的入口，但并不关心接入数据源的类型；而Plumber类似于Kafka Connect Sink，定义了数据的出口，也不关心最终输出到哪里
- **Stream Push**  如果采用Stream Push策略，我们需要建立一个“copy service”，负责从数据源中拉取数据并生成Indexing Service Tasks，从而将数据“推入”到Druid中，我们在druid_0.9.1版本之前一直使用的是这种模式，不过这种模式需要外部服务Tranquility，Tranquility组件可以连接多种流式数据源，比如Spark-Streaming、Storm以及Kafka等，所以也产生了Tranquility-Storm、Tranquility-Kafka等外部组件。

## Druid数据存储
# CUDA流

CUDA流：一系列异步的CUDA操作，这些操作按照主机代码的顺序在设备上执行。这些操作包括：

- 在主机与设备间传递数据
- 内核启动
- 大多数由主机发起，设备执行的命令

流分两种类型：

- 默认流：隐式声明中的流(又叫空流)，
- 非空流：显示声明的流，核函数如果没有在启动时显示声明运行在某个流上的话，那么就运行在默认流上。

## 非空流的创建

首先创建4个流：

```C++
#define NSTREAM 4
int n_streams = NSTREAM;
cudaStream_t *stream = (cudaStream_t *)malloc(	n_streams*sizeof(cudaStream_t));
for(int i=0;i<n_streams;i++)
{
    CHECK(cudaStreamCreate(&(streams[i])));
}
```

定义一个核函数

```C++
__global__ void kernel_()
{
    double sum = 0.0;
    for(int i=0; i<N; i++)
    {
        sum = sum + tan(0.1)*tan(0.1);
    }
}
```

## 非空流的启动

启动：`kernel_<<grid,block,shareMemSize,streams>>>()`

```C++
// 默认流，只在一个default流上
kernel_1<<<grid,block>>>();
kernel_2<<<grid,block>>>()
kernel_3<<<grid,block>>>()
kernel_4<<<grid,block>>>()
    
// 非空流，只在一个default流上
kernel_1<<<grid,block,0,streams[0]>>>();
kernel_2<<<grid,block,0,streams[1]>>>()
kernel_3<<<grid,block,0,streams[2]>>>()
kernel_4<<<grid,block,0,streams[3]>>>()
```

## 非空流的销毁

```c++
//释放资源
for(int i=0;i<nStreams;i++)
{
    CHECK(cudaStreamDestroy(streams[i]));
}
```

# 将内核执行和数据传输重叠

以矩阵相加为例，将矩阵切分成**四个切片**，放在不同的流上，同时将一个切片的传输与另一个切片的核函数计算并发执行。

实现传输和核函数执行的并发：

- 不再使用之前的`cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice)`
- 采用`cudaMemcpyAsync(&d_A[ioffset], &h_A[ioffset], iBytes,cudaMemcpyHostToDevice, stream[i]))`

首先，定义矩阵相加的内核函数：

```c++
__global__ void sumArrays(float *A, float *B, float *C, const int N)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N)
    {
        for (int i = 0; i < N; ++i)
        {
            C[idx] = A[idx] + B[idx];
        }
    }
}
```

之前的做法是在主机端初始化两个矩阵，再将数据整体搬移到设备端，再利用线程分别取数，所有线程执行完毕后，再将数据整体搬移到主机端。

这次的做法是将这个矩阵分成4个切片，分别放入4个流中，并发执行，同时要求每一个流单独的执行数据传输和内核执行，不同流之间要将内核执行与数据传输进行重叠。利用如下方式将数据分成四块，分别通过四个流的调度将并行的传输数据，同时并行的执行相加的内核程序。

```C++
#define NSTREAM 4
sumArrays<<<grid, block>>>(d_A, d_B, d_C, nElem);    // 在空流上执行不分解的核函数
int iElem = nElem / NSTREAM;    // 数据分成了4份
for (int i = 0; i < NSTREAM; ++i)
    {
        int ioffset = i * iElem;    // 设置不同的流处理切片内存首地址的步长
// 将不同区域的数据传输放在不同的流上
        CHECK(cudaMemcpyAsync(&d_A[ioffset], &h_A[ioffset], iBytes,
                              cudaMemcpyHostToDevice, stream[i]));
        CHECK(cudaMemcpyAsync(&d_B[ioffset], &h_B[ioffset], iBytes,
                              cudaMemcpyHostToDevice, stream[i]));
// 在不同的流上执行不同切片的并行相加
        sumArrays<<<grid, block, 0, stream[i]>>>(&d_A[ioffset], &d_B[ioffset],
                &d_C[ioffset], iElem);
// 将不同切片的数据在不同的流上执行传输回主机端的操作
        CHECK(cudaMemcpyAsync(&gpuRef[ioffset], &d_C[ioffset], iBytes,
                              cudaMemcpyDeviceToHost, stream[i]));
    }
```

思想是：根据流的个数，把数据分成不重叠的多份，执行完后，再把各个执行的结果再调用核函数执行一下。


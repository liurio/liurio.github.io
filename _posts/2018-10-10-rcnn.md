## R-CNN的核心思想

把图片区域的内容送给深度网络，然后提取出深度网络某层的特征，并用这个特征判断是什么物体，最后再对是物体的区域进行微调调整。

## R-CNN的主要步骤
- 候选区域的选择(区域提名)：通过selective Search方法从原始的图片中提取2000个左右的候选区域。
- 区域大小的归一化：把所有的候选区域缩放或固定大小(227*227)，作为CNN的固定输入
- 特征提取：通过CNN提取特征，输出固定维度
- 分类和回归：做法是**特征层的基础上添加连个fc层，一个是svm分类；另一个bbox层用线性回归微调边框位置和大小**
    - 分类：对上一步的输出向量进行分类，此时用的分类方法是SVM
    - 边框回归：是对RP进行纠正的线性回归算法，目的是让RP提取到的窗口与目标窗口更加吻合。
    
![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/RCNN%E5%8E%9F%E7%90%86.png)

## 边框回归
### 为什么要边框回归？

![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/IOU.png)

![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/plane.png)

绿色的框为飞机的Ground Truth，红色的框是提取的Region Proposal。那么即便红色的框被分类器识别为飞机，但是由于红色的框定位不准(IoU<0.5)，那么这张图相当于没有正确的检测出飞机。如果我们能对红色的框进行微调，使得经过微调后的窗口跟Ground Truth更接近，这样岂不是定位会更准确。确实，Bounding-box regression 就是用来微调这个窗口的。

### 回归/微调的对象是什么？

![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/%E5%9B%9E%E5%BD%92%E7%AA%97%E5%8F%A3.png)

### 边框回归具体内容

经过何种变换才能从P状态变为G'呢？比较简单的思路：**先做平移再做缩放**

![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/bb%E5%B9%B3%E7%A7%BB.png)

![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/bb%E7%BC%A9%E6%94%BE.png)

注意：只有当Proposal和Ground Truth比较接近时(线性问题), 才能将其作为训练样本训练我们的线性回归模型，否则会导致训练的回归模型不work（当Proposal跟GT离得较远，就是复杂的非线性问题了，此时用线性回归建模显然不合理）

**线性回归**就是给定输入的特征向量X，学习一组参数W，使得经过线性回归后的值跟真实值Y(Ground Truth)非常接近。即。那么Bounding-box中我们的输入以及输出分别是什么呢？

- **输入：**
​					$$ RP--->P=(P_x,P_y,P_w,P_h) $$

输入就是这四个数值吗？其实真正的输入是这个窗口对应的CNN特征，也就是R-CNN中的Pool5feature（特征向量）。(注：训练阶段输入还包括 Ground Truth)

- **输出：**
​				$$ (d_x(P),d_y(P),d_w(P),d_h(P)) \space or \space (\triangle x,\triangle y,S_w,S_h)$$

我们的最终输出不应该是Ground Truth吗？是的，但是有了这四个变换我们就可以直接得到Ground Truth，这里还有个问题，根据上面4个公式我们可以知道，得到的并不是真实值G，而是预测值G'。

的确，这四个值应该是经过 Ground Truth 和Proposal计算得到的真正需要的平移量(t_x,t_y)和尺度缩放(t_w,t_h)。

​					$$t_x=\frac{G_x-P_x}{P_w},\space\space\space$$

​					$$t_y=\frac{G_y-P_y}{P_h}$$

​					$$t_x=log\frac{G_w}{P_w},\space\space\space$$

​					$$t_y=log\frac{G_h}{P_h}$$

那么目标函数可以表示为

​					$$d_*(P)=w_*^T\phi_5(P) $$

其中*代表x,y,w,h也就是每一个变换对应一个目标函数，w的后一项代表输入Proposal的特征向量，d_(P)代表预测值。我们要让预测值和真实值的误差最小，因此损失函数loss为：

​					$$Loss=\sum_i^N{(t_*^i-w_*^T\phi_5(P^i))^2}$$

函数的优化目标函数为：

​					$$w_*=argmin_w\sum_i^N{(t_*^i-w_*^T\phi_5(P^i))^2}+\lambda \| w_* \|^2$$

利用梯度下降法或者最小二乘法就可以求解。

## 从一张图中找多个区域
![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/%E9%9D%9E%E6%9E%81%E5%A4%A7%E5%80%BC%E6%8A%91%E5%88%B6.png)

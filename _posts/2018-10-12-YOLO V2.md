code: [https://github.com/hizhangp/yolo_tensorflow](https://github.com/hizhangp/yolo_tensorflow)

目标检测-YOLOv2原理与实现(附YOLOv3) [https://zhuanlan.zhihu.com/p/35325884](https://zhuanlan.zhihu.com/p/35325884)

YOLO V2和YOLO9000是两个不同的模型，论文一并提出，YOLO V2 + join training = YOLO9000
[https://arxiv.org/pdf/1612.08242.pdf](https://arxiv.org/pdf/1612.08242.pdf)

## YOLO V2
YOLO V2在V1的基础上主要做了以下改进：

![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/yolo-v2.png)

### Batch Normalization(批量规范化)
因为对数据的预处理(统一格式、均衡化、去噪等)能够提高训练速度，提升训练效果。
- **作用**
    - 神经网络的每层分布总是变化，通过标准化均衡输入分布，加快训练速度，可以设置较大的学习率和衰减率。
    - 通过规范化输入，降低激活函数在特定区间达到饱和状态。
    - 输入规范化对应样本正则化，在一定程度上可以替代Dropout

- **做法**

   在做卷积池化之后，激活函数之前，对每个数据输出进行规范化(均值为0，方差为1)

   ![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/yolo-v2-1.png)
>公式的第一部分就是正态分布标准化的公式；第二部分引入了附件参数`\lambda`和`\beta`？
>因为简单的归一化，相当于只使用了激活函数中近似线性的部分(如下图红线所示)，破坏了原始数据的特征分布，会降低模型的表达能力。

![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/batch-normalization.jpg)

>
>这两个参数需要通过训练得到，具体的前向后向传播过程训练如下：

![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/paramater.jpg)

更多BN：https://www.zhihu.com/question/38102762
### 预训练尺寸
yolov1也在Image-Net预训练模型上进行fine-tune，但是预训练时网络入口为224 x 224，而fine-tune时为448 x 448，这会带来预训练网络与实际训练网络识别图像尺寸的不兼容。yolov2直接使用448 x 448的网络入口进行预训练，然后在检测任务上进行训练，效果得到3.7%的提升。
### 更多网格划分
YOLOV2为了提升小物体检测效果，减少网络中的pooling层数目，使最终的特征图尺寸更大，如输入的是`416*416`，则输出的是`13*13*125`，其中`13*13`为最终的特征图，即原图分格的个数，125是每个格子中的边框构成`(5*(classes+5)`，特征图尺寸取决于原图尺寸，但特征图尺寸必须为奇数，以此保存中间有一个位置能看到原图中心处的目标。
### 全卷积网络
为了使网络能够接受更多尺寸的输入图像，yolov2除去了v1网络结构的全连接层，因为全连接层必须要输入固定尺寸的特征向量；做法是将网络变成一个全卷积网络。
### 新网络
下图为不同基础网络结构做分类任务所对就的计算量，横坐标为做一次前向分类任务所需要的操作数目。可以看出使用的darknet-19作为基础训练网络(共19个卷积层)，能在保持高精度下快速计算。而SSD使用VGC-16作为基础网络，VGC-16虽然精度和darknet-19差不多，但是速度就慢多了。
darknet-19：https://pjreddie.com/darknet/imagenet/

![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/darknet-19.png)

整个darknet-19的网络借鉴了很多其他网络的概念，主要使用`3*3`卷积并在pooling之后channel数加倍；global average pooling(全局平均池化)替代全连接做预测分类，并在`3*3`卷积之间使用`1*1`卷积压缩特征表示(Network-in-Network)https://arxiv.org/pdf/1312.4400.pdf ; 使用batch normalization来提高稳定性，加速收敛，对模型正则化。整个网络包括19个conv + 5 max pooling

![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/darknet-19-avg.png)

从结构中看出，使用全局平均池化层替换了全连接层，简单说一下**global average pooling**
- 全连接层缺点：参数量过大、容易引起过拟合
- 全局平均池化：将最后一层的特征图进行整张图的一个均值池化，形成一个特征点，将这些特征点组成最后的特征向量；
- 例如：最后的一层的数据是10个`6*6`的特征图，global average pooling是将每一张特征图计算所有像素点的均值，输出一个数据值，这样10 个特征图就会输出10个数据点，将这些数据点组成一个`1*10`的向量的话，就成为一个特征向量，就可以送入到softmax的分类中计算了

![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/global%20average%20pooling.png)

### Convolutional With Anchor Boxes(anchor机制)
yolov2为了提高召回率和精度，使用Faster RCNN中的anchor机制。

V2借鉴了Faster RCNN的思想预测了bbox的偏移，移除了全连接层，并且删掉了一个pooling层使特征分辨率更大一些。另外调整了网络的输入`448 -> 416`使得位置坐标是奇数且只有一个中心点。YOLO v2对Faster R-CNN的手选先验框方法做了改进,采样k-means在训练集bbox上进行聚类产生合适的先验框.
Faster的 Anchor 机制又一次得到印证，与SSD一样，Anchor建立了和原始坐标的对应关系：定义了不同scale的宽高比，一个中心对应k个不同尺度和宽高比的boxes。

![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/yolov2-anchor.jpg)

- YOLO-V1:`S*S*(B*5+C) -> 7*7*(2*5+20)`
    - 其中B对应box数量，5对应x,y,w,h,c
    - 每个grid只能预测两个box，这两个box公用一个分类结果(20 classes)
- YOLO-V2:`S*S*K*(5+C) -> 13*13*5*(5+20)`
    - 分辨率改成了13*13，更细的格子划分对小目标适应更好
- SSD:`S*S*K*(4 + C)  -> 7*7*6*(4+21)`
    - 对应C=21，代表20种分类类别和一种 背景类

### 维度聚类
anchor中的k到底取多少呢，SSD选取了6，方法是：利用k-means算法离线对conv和coco数据集中的目标形状进行计算，其采用的距离函数是`d(box,centoid)=1-IoU(box,centoid)`,发现k=5是个比较好的选择。

k-means算法代码实现参考:k_means_yolo.py.：https://github.com/PaulChongPeng/darknet/blob/master/tools/k_means_yolo.py

### Fine-Grained Features
YOLO V2的输入图片是`416*416`，进过5次maxpooling之后得到`13*13`大小的特征图，并以此特征图采用卷积做预测。`13*13`大小的特征图对检测大物体足够了，但是对于小物体还需要更新的特征图(Fine-Grained Features)。因此SSD使用了多尺度的特征图来分别检测不同大小的物体，前面更精细的特征图可以用来预测小物体。YOLO V2所利用的是`26*26`大小的特征图(最后一个maxpooling的输入)，对于darknet-19来说就是大小为`26*26*512`的特征图。passthrough层与ResNet网络的shortcut类似，以前面更高分辨率的特征图为输入，然后将其连接到后面的低分辨率的特征图上。前面的特征图维度是后面的特征图维度的2倍，passthrough层抽取前面层的每个`2*2`的局部区域，然后将其转化为channel的维度，对与`26*26*512`的特征图，经过passthrough层处理之后就变成了`13*13*2048`的新特征图(特征图大小降低4倍，而channels增加4倍，下图为一个实例)，这样就可以与后面的`13*13*1024`的特征图连接在一起，形成`13*13*3072`大小的特征图，然后在此特征图基础上卷积做预测。在YOLO的C源码中，passthrough层称为reorg layer。在TensorFlow中，可以使用tf.extract_image_patches或者tf.space_to_depth来实现passthrough层：
```python
out = tf.extract_image_patches(in, [1, stride, stride, 1], [1, stride, stride, 1], [1,1,1,1], padding="VALID")
// or use tf.space_to_depth
out = tf.space_to_depth(in, 2)
```

![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/yolov2-passthrough.jpg)
### 直接位置预测
直接anchor box回归会导致模型不稳定，中心点可能会出现在图像的任意位置，有可能导致回归过程震荡，甚至无法收敛：

![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/yolov2-2.png)

其中，`(x,y,w,h)`为实际的位置。`x_a,y_a,w_a,h_a`为候选框的位置，`t_x,t_y,t_w,t_h`为预测的位置

针对这个问题，在预测位置时采用了强约束方法：
- 对应cell距离左上角的边距是`C_x,C_y`,`\sigma`定义为激活函数，将函数值约束在[0,1]的范围之内，用来确保该中心不会偏离该cell；
- 预定anchor对应的宽高`(p_w,p_h)`，预测 Location 是相对于Anchor的宽高 乘以系数得到

![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/yolo-location.jpg)
### 多尺度训练
原来的YOLO网络使用固定的`448 * 448`的图片作为输入，现在加入anchor boxes后，输入变成了`416 * 416`。目前的网络只用到了卷积层和池化层，那么就可以进行动态调整（意思是可检测任意大小图片）。作者希望YOLOv2具有不同尺寸图片的鲁棒性，因此在训练的时候也考虑了这一点。

不同于固定输入网络的图片尺寸的方法，作者在几次迭代后就会微调网络。没经过10次训练（10 epoch），就会随机选择新的图片尺寸。YOLO网络使用的降采样参数为32，那么就使用32的倍数进行尺度池化`\{320,352,…,608\}`。最终最小的尺寸为`320 * 320`，最大的尺寸为`608 * 608`。接着按照输入尺寸调整网络进行训练。

这种机制使得网络可以更好地预测不同尺寸的图片，意味着同一个网络可以进行不同分辨率的检测任务，在小尺寸图片上YOLOv2运行更快，在速度和精度上达到了平衡。
​     
## YOLO9000
采用了wordtree的方法，综合ImageNet数据集和COCO数据集训练YOLO9000，使之能够识别超过9000多种物品。

目前基于深神经网络的检测系统能够识别的物品种类很少，原因在于：相对物品分类数据集，目标检测数据集中的物品种类少。标记目标识别数据集所耗费的精力远大于标记物品分类数据集。物品分类数据集很大，而目标识别数据集就很小了。

作者提出了一种结合不同类型数据集的方法。基于该方法，作者提出了一种新的联合训练方法，结合目前分类数据集的优点，将其应用于训练目标检测模型。模型可以从目标检测数据集中学会定位目标，同时从物品分类数据集中学会识别更多的种类，增强模型的鲁棒性。几个重要的点
- 不同的数据来源的分类可能有包含关系，通过WordNet构造WordTree，实现对label的树形表示；
- 当样本标签属于叶子结点时，父亲节点可能被标记为正样本；当样本标签不是叶子节点时，值针对非叶子结点和其父亲进行反向传播。
- 对分类图片，此时得到一个置信度最高的bbox，将其当做一个负样本进行反向传播；
- 混合训练实现了网络可以检测、识别出一些以前没有的object

![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/yolov-9000.png)

### yolo v2训练
YOLOv2的训练主要包括三个阶段。第一阶段就是先在ImageNet分类数据集上预训练Darknet-19，此时模型输入为 224\times224 ，共训练160个epochs。然后第二阶段将网络的输入调整为 448\times448 ，继续在ImageNet数据集上finetune分类模型，训练10个epochs，此时分类模型的top-1准确度为76.5%，而top-5准确度为93.3%。第三个阶段就是修改Darknet-19分类模型为检测模型，并在检测数据集上继续finetune网络。网络修改包括（网路结构可视化）：移除最后一个卷积层、global avgpooling层以及softmax层，并且新增了三个 3\times3\times2014卷积层，同时增加了一个passthrough层，最后使用 1\times1 卷积层输出预测结果，输出的channels数为： \text{num_anchors}\times(5+\text{num_classes}) ，和训练采用的数据集有关系。由于anchors数为5，对于VOC数据集输出的channels数就是125，而对于COCO数据集则为425。

下面图分别是**yolov2训练的三个阶段**和**yolov2的结构图**

![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/yolov2-train.jpg)

![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/yolov2-net.jpg)

### yolo v2损失函数loss

![image](https://raw.githubusercontent.com/liurio/deep_learning/master/img/yolov2-loss.jpg)

首先**W**和**H**分别是特征图(`13*13`)的宽和高，而**A**是指先验框的数目5，各个`\lambda`值是各个loss部分的权重系数。第一项loss是计算背景background的置信度误差，但是哪些预测框来预测背景呢，需要先计算各个预测框和所有groud truth的IOU值，并且取最大的IOU,如果该值小于0.6，那么这个框就预测为background。需要计算noobj的置信度误差；

第二项是计算先验框和预测宽的坐标误差(包括x,y,w,h)，但是只在前12800个iterations间计算，我觉得这项应该是在训练前期使预测框快速学习到先验框的形状。

第三项是计算某个ground truth匹配 的预测框的各部分loss值，包括坐标误差，置信度误差以及分类误差。

先说一下匹配原则，对于某个ground truth，首先要确定其中心点要落在哪个cell上，然后计算这个cell的5个先验框与ground truth的IOU值（YOLOv2中bias_match=1），计算IOU值时不考虑坐标，只考虑形状，所以先将先验框与ground truth的中心点都偏移到同一位置（原点），然后计算出对应的IOU值，IOU值最大的那个先验框与ground truth匹配，对应的预测框用来预测这个ground truth。在计算obj置信度时，target=1，但与YOLOv1一样而增加了一个控制参数rescore，当其为1时，target取预测框与ground truth的真实IOU值（cfg文件中默认采用这种方式）。对于那些没有与ground truth匹配的先验框（与预测框对应），除去那些Max_IOU低于阈值的，其它的就全部忽略，不计算任何误差。这点在YOLOv3论文中也有相关说明：YOLO中一个ground truth只会与一个先验框匹配（IOU值最好的），对于那些IOU值超过一定阈值的先验框，其预测结果就忽略了。这和SSD与RPN网络的处理方式有很大不同，因为它们可以将一个ground truth分配给多个先验框。尽管YOLOv2和YOLOv1计算loss处理上有不同，但都是采用均方差来计算loss。另外需要注意的一点是，在计算boxes的 w 和 h 误差时，YOLOv1中采用的是平方根以降低boxes的大小对误差的影响，而YOLOv2是直接计算，但是根据ground truth的大小对权重系数进行修正：l.coord_scale * (2 - truth.w*truth.h)（这里w和h都归一化到(0,1))，这样对于尺度较小的boxes其权重系数会更大一些，可以放大误差，起到和YOLOv1计算平方根相似的效果

```python
// box误差函数，计算梯度
float delta_region_box(box truth, float *x, float *biases, int n, int index, int i, int j, int w, int h, float *delta, float scale, int stride)
{
    box pred = get_region_box(x, biases, n, index, i, j, w, h, stride);
    float iou = box_iou(pred, truth);
   
    // 计算ground truth的offsets值
    float tx = (truth.x*w - i);  
    float ty = (truth.y*h - j);
    float tw = log(truth.w*w / biases[2*n]);
    float th = log(truth.h*h / biases[2*n + 1]);

    delta[index + 0*stride] = scale * (tx - x[index + 0*stride]);
    delta[index + 1*stride] = scale * (ty - x[index + 1*stride]);
    delta[index + 2*stride] = scale * (tw - x[index + 2*stride]);
    delta[index + 3*stride] = scale * (th - x[index + 3*stride]);
    return iou;
}
```